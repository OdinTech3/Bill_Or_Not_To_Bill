---
title: "R Notebook"
output: html_notebook
---

```{r}
library(readxl)
library(dplyr)
library(plyr)
library(tm)
library(keras)
```


```{r}
non_bill_df <- as.data.frame (read.csv("C:\\SPRING SEM\\Projects\\December Non-bill calls.csv")) 
non_bill_df <- non_bill_df[-c(3)] 
```


```{r}
non_bill_df$SR.Device = as.numeric((factor(non_bill_df$SR.Device, levels = unique(non_bill_df$SR.Device))))
non_bill_df$Activity.Type = as.numeric((factor(non_bill_df$Activity.Type, levels = unique(non_bill_df$Activity.Type))))
non_bill_df$SR.Number = as.numeric((factor(non_bill_df$SR.Number, levels = unique(non_bill_df$SR.Number))))
non_bill_df$SR.Type = as.numeric((factor(non_bill_df$SR.Type, levels = unique(non_bill_df$SR.Type))))
non_bill_df$Cash.Vendor...Consumable.Contracts = as.numeric((factor(non_bill_df$Cash.Vendor...Consumable.Contracts, levels = unique(non_bill_df$Cash.Vendor...Consumable.Contracts))))

```


```{r}
head(non_bill_df)
```

```{r}
non_bill_df %>% select(Call.Text, Activity.Trouble.Code) -> nonbilldf1
nonbilldf1 <- as.vector(nonbilldf1)
```

```{r}
samples <- nonbilldf1$Call.Text %>%
  tolower() %>%
  removePunctuation() %>%
  removeNumbers() %>%
  stripWhitespace() %>%
  removeWords(stopwords("en"))
```

```{r}


# Creates a tokenizer, configured to only take into account the 1,000 
# most common words, then builds the word index.
tokenizer <- text_tokenizer(num_words = 1000) %>%
  fit_text_tokenizer(samples)

# Turns strings into lists of integer indices
sequences <- texts_to_sequences(tokenizer, samples)

# You could also directly get the one-hot binary representations. Vectorization 
# modes other than one-hot encoding are supported by this tokenizer.
one_hot_results <- texts_to_matrix(tokenizer, samples, mode = "binary")

# How you can recover the word index that was computed
word_index <- tokenizer$word_index

cat("Found", length(word_index), "unique tokens.\n")
```
```{r}
samp<-floor(0.75*nrow(nonbilldf1))
train_non<- sample(seq_len(nrow(nonbilldf1)), size = samp)
train <- nonbilldf1[train_non, ]
test <- nonbilldf1[-train_non, ]
```

```{r}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = word_index, output_dim = 32) %>%
  layer_simple_rnn(units = 32) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  train,
  epochs = 5,
  batch_size = 128,
  validation_split = 0.2
)
```
